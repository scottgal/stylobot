Path-Based Escalation Policies

Path-based escalation allows you to define different detection pipelines for different parts of your application,
instead of a single global policy.

This enables high-friction protection where it matters (e.g. /login, /checkout) and minimal overhead where it doesn’t (
e.g. /health, /rss, /public-assets).

How It Works

You define one or more named policies (e.g. default, loginStrict, apiHighConfidence, publicRelaxed).

Each policy can override:

Which detectors run, and in what sequence

Which triggers apply

Whether the fast path can early-abort

How deep the slow path should go

Thresholds for escalation to AI / ML detectors

You map policies to paths using globbing or regex:

{
"BotDetection": {
"Policies": {
"default": { "UseFastPath": true },
"loginStrict": { "UseFastPath": true, "ForceSlowPath": true, "EscalateToAI": true }
},
"PathPolicies": {
"/login/*": "loginStrict",
"/checkout/*": "loginStrict",
"/api/*": "default",
"/public/*": "relaxed"
}
}
}

Why This Matters

No more one-size-fits-all detection.

Reduce overhead for public endpoints.

Increase accuracy and security around risky actions.

Enables per-team, per-product, or per-environment customization.

Defaults

If you do nothing:

All endpoints use the default policy with a balanced fast/slow path and sane thresholds.

Configurable Detector Weights

Every detector in the blackboard pipeline contributes a weighted confidence delta.
By default, these weights are tuned to be well-balanced, safe, and effective for the majority of workloads.

But for advanced users, each detector’s weight is configurable.

Concept

Each detector emits:

ConfidenceDelta × Weight

You can tweak each detector’s influence without modifying code.

Example
{
"BotDetection": {
"Weights": {
"UserAgent": 1.0,
"IpReputation": 1.5,
"HeaderAnalysis": 0.8,
"InconsistencyDetector": 2.0,
"Behavioral": 1.2,
"AIDetector": 3.0
}
}
}

Interpretation

Boost weight → detector has more impact when triggered

Lower weight → detector’s signal is treated as weaker

Zero weight → detector effectively disables itself but still contributes signals to others

Negative weight → (supported) can invert a detector’s contribution if a user really wants to bias the system in a
specific direction

Philosophy: Panacea Defaults

Your library ships with:

Well-researched default weights

Documentation for every detector, explaining:

What it measures

Typical signal strength

When to increase or decrease its weight

Potential false-positive impact

Expected performance cost

The goal:

You never need to tune weights.
But if you want to, you get full access—with zero penalties.

Per-Policy Weight Overrides

Path policies can override weights too:

{
"/login/*": {
"Policy": "loginStrict",
"Overrides": {
"Weights": {
"Behavioral": 2.0,
"InconsistencyDetector": 3.5
}
}
}
}

This enables “hyper-strict” behaviour on sensitive endpoints without globally changing your system’s behaviour.

CLI Descriptions (Drop-In Text)
--list-policies

Show all configured escalation policies and which paths they map to.

--describe-policy <name>

Show detectors, weights, thresholds, triggers, and overrides for a given policy.

--set-weight <detector>=<value>

Override an individual detector weight globally.

--set-policy-weight <policy> <detector>=<value>

Override detector weight only within a specific escalation policy.

--explain-weighting

Explains how weighted confidence aggregation works, with real examples from your config and detectors.

Detector Weighting System (Fully Optional, Sensible Defaults)

Every detector in the pipeline contributes evidence, not verdicts.
The weighting system allows power users to fine-tune how these contributions influence the final risk score.

Why weights exist

To keep the core engine stable and predictable

To allow users to emphasise detectors that matter for their environment

To let advanced users incorporate new detectors (including AI models) without modifying the library

To ensure small deployments and large ones behave sensibly on day one

You never need to set these.

The defaults are chosen to be safe, conservative, and production-ready.

But if your environment differs, you can tune them.
How Weights Work

Each contribution generated by a detector contains:

a confidence delta (evidence strength)

a weight (importance / trust level)

Final risk score = weighted average of all contributions.

Example configuration:

"BotDetection": {
"Weights": {
"UserAgentDetector": 1.0,
"HeaderDetector": 1.0,
"IpReputationDetector": 1.5,
"BehaviorDetector": 2.0,
"AIDetector": 0.8
}
}

If omitted, the defaults apply.

Design Goals (Open Source Philosophy)

1. Stable defaults for everyone

If you install the package and never touch a single setting, you get a high-quality detection engine.

The weighting system is optional, not required.

2. Fully transparent, well-documented tuning

Weights aren’t magical.
Each detector will have documentation describing:

what signals it contributes

what kind of environments benefit from raising/lowering it

what false-positive patterns might be affected

This empowers users rather than locking them out.

3. No behavioural drift across versions

Because weights live in configuration (not code):

your detection logic is stable

major-version compatibility is respected

upgrades don’t change semantics

users choose when and how behaviour changes

This is incredibly important for production deployments.

4. Hardware-agnostic scaling

This is subtle but powerful:

If a user installs a large Ollama model (like 70B), they can raise the weight of the AI detector to make use of it.

If they run on a Raspberry Pi, they can set that weight lower or leave it at default.

The library does not assume AI is always available or useful.

This aligns with OSS principles:
You meet users where they are.
You don’t force them to adapt to your assumptions.

5. Encourages community contributions

Because weights are user-adjustable:

new detectors can be added without destabilising the ecosystem

contributors don’t need to hit some “perfect” balance

users can experiment, tune, and share their config presets

No gatekeeping.
No hidden heuristics.
No baked-in assumptions.

Why This Fits Your Ecosystem Perfectly

Every one of your projects follows the same ethos:

clear defaults

deep configurability

clean architecture

user empowerment

"works out of the box, scales if you want it to"

Weights are simply the natural extension of that philosophy.

They don’t upsell anything.
They aren’t a feature flag for “enterprise edition.”
They’re just good open-source engineering.

olicies are no longer static configs; they're workflow components.

Policies are now first-class workflow units.
A policy is not just “which detectors to run” — it is a flow that can react to signals, branch, escalate, or delegate to
other flows.

This transforms the detection system into a policy-driven workflow engine.

Core Idea

A Policy is a named workflow consisting of:

stages (fast path, slow path, AI path, behavioural path)

conditions (triggered by signals, risk levels, states)

actions (continue, escalate, switch policy, challenge, block)

transitions (to other policies)

Policies can call each other — enabling flow chaining.

Why This Matters

Most bot detection systems are rigid:
“Run detectors X,Y,Z with some weights.”

Your system is flexible:

A policy can say things like:

“If UA looks like a validated search crawler → switch to ‘GoodBotPolicy’.”

“If behavioural anomalies spike → jump to ‘ChallengePolicy’.”

“If risk exceeds 0.8 AND headers mismatch UA → escalate to ‘DeepInspectionPolicy’.”

This creates a stateful, reactive, event-driven detection engine, not a bag of heuristics.

What a Policy Looks Like
"Policies": {
"Default": {
"FastPath": ["UserAgent", "Header"],
"SlowPath": ["IpReputation", "Behavioural", "Inconsistency"],
"AiPath": ["LlmAnalyzer"],
"Transitions": [
{
"WhenRiskExceeds": 0.8,
"GoTo": "DeepInspection"
},
{
"WhenSignal": "VerifiedGoodBot",
"GoTo": "AllowBotPolicy"
}
]
},

"DeepInspection": {
"FastPath": [],
"SlowPath": ["IpReputation", "Inconsistency"],
"AiPath": ["LlmAnalyzer", "Heuristic"],
"Transitions": [
{ "WhenRiskBelow": 0.4, "GoTo": "Default" },
{ "WhenRiskExceeds": 0.95, "GoTo": "BlockPolicy" }
]
}
}

Policy State Machine

Every policy is effectively a state in a detection state machine.

stateDiagram-v2
Default --> DeepInspection: risk ≥ 0.8
DeepInspection --> BlockPolicy: risk ≥ 0.95
DeepInspection --> Default: risk < 0.4
Default --> GoodBotPolicy: VerifiedGoodBot signal
GoodBotPolicy --> Default: reputation normalized

Signal-Driven Policy Transitions

This integrates naturally with the blackboard architecture:

Policies transition on:

signals published by detectors

risk thresholds

reputation state changes

behavioural anomalies

AI classification results

inconsistency escalations

This means a policy reacts immediately as new evidence appears.

Example:

if (bb.Signals.ContainsKey(SignalKeys.InconsistencyDetected))
return PolicyTransition.To("DeepInspection");

Why This Is Powerful

1. Composable

You define small, clear policies — then combine them.

2. Extensible

Users can add their own detectors and attach them to any policy flow.

3. Self-optimizing

Policies can evolve via:

reputation learning

drift detection

LLM-generated policies (future add-on)

4. Perfect for host applications

Different endpoints can map to different flows:

"PathPolicies": {
"/login": "DeepInspection",
"/api/payment": "ChallengePolicy",
"/public-content": "Default"
}

5. Zero vendor lock-in

This is pure open architecture, not SaaS pattern gating.

Example: Endpoint-Specific Escalation

A login page might escalate quickly:

FastPath → SlowPath → Behaviour → AI → Challenge

A static asset endpoint might use a lightweight flow:

FastPath → Cache

An admin panel might force strict verification:

FastPath → Behaviour → AI → BlockSuspicious

Future-Proof: LLM-Derived Policies

Because policies are declarative, a small local LLM can:

interpret natural language policies

generate policy configuration blocks

adjust weights

propose new transitions

Example prompt:

“Treat all unknown crawlers like high-risk unless they prove consistency.”

Your system converts that into a policy flow.

Summary for CLI / README
🧩 POLICY WORKFLOWS
Policies are now composable workflow components. Each policy defines its
own detection paths and can transition to other policies based on
signals, risk thresholds, inconsistencies, behavioural anomalies, or
AI analysis.

This turns the detector into a small workflow engine instead of a fixed
pipeline. Policies can be assigned per endpoint, per service, or
dynamically during detection.

Default policies work out of the box — advanced users can build
custom flows without modifying library code.

If you want, I can write: